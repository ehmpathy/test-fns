# blackbox criteria: slowtest reporter

## usecase.1 = diagnose slow tests via terminal

```
given('a jest test suite with slow tests')
  when('tests complete')
    then('terminal displays slowtest report after standard output')
      sothat('developers see slow tests without extra commands')
    then('report shows files sorted by duration, slowest first')
    then('report shows duration for each file in human-readable format')
      sothat('developers can quickly scan for problem files')
    then('report respects `top` limit if configured')
      sothat('large suites dont flood terminal')

given('a vitest test suite with slow tests')
  when('tests complete')
    then('terminal displays identical report format as jest')
      sothat('developers have consistent experience across runners')

given('a test suite with all tests below slow threshold')
  when('tests complete')
    then('report displays with all files shown')
    then('no slow indicators appear')

given('a test suite with tests above slow threshold')
  when('tests complete')
    then('report indicates count of files above slow threshold')
    then('slow files are visually marked in report')
```

## usecase.2 = diagnose with block-level hierarchy

```
given('tests structured with given/when/then blocks')
  when('tests complete')
    then('report shows nested hierarchy: file > given > when > then')
      sothat('developers pinpoint exact slow block')
    then('each level shows its own duration')
    then('indentation reflects nest depth')

given('tests with nested given inside given')
  when('tests complete')
    then('full hierarchy preserved in output')

given('tests with useBeforeAll in given block')
  when('tests complete')
    then('setup time attributed to the given block duration')
    then('individual then blocks show only their execution time')
      sothat('developers distinguish setup cost from test cost')

given('a block with hooks (beforeAll, afterAll, beforeEach, afterEach)')
  when('tests complete')
    then('block duration includes hook time')
    then('hook time reported separately as: blockDuration - sum(childDurations)')
      sothat('developers see where time goes within a block')
    then('hook time labeled as "hooks" in report')

given('a given block where hooks take most of the time')
  when('tests complete')
    then('report shows given block duration as total')
    then('report shows hooks duration derived from delta')
    then('report shows child when blocks with their own durations')
      sothat('developers identify that hooks are the bottleneck')

given('tests with plain describe blocks (not given/when)')
  when('tests complete')
    then('file-level duration still reported')
    then('block-level breakdown not available for plain describe')
      sothat('system degrades gracefully for non-bdd tests')
```

## usecase.3 = export json for tools

```
given('output path configured')
  when('tests complete')
    then('json file written to configured path')
    then('json includes generated timestamp')
    then('json includes summary: total duration, file count, slow count')
    then('json includes array of files with path and duration')

given('format set to default (full)')
  when('tests complete')
    then('json includes nested blocks array for each file')
    then('each block has type, name, duration')
    then('each block has hookDuration derived from duration - sum(children)')
    then('blocks nest recursively to match test hierarchy')

given('format set to shard')
  when('tests complete')
    then('json contains only version and files map')
    then('files map has path as key, duration in ms as value')
      sothat('shard tools can parse with minimal overhead')

given('output directory does not exist')
  when('tests complete')
    then('directory created automatically')
    then('json file written successfully')
```

## usecase.4 = track regressions against baseline

```
given('baseline file configured and exists')
  when('tests complete')
    then('report shows delta vs baseline for each file')
    then('positive delta shown as regression indicator')
    then('negative delta shown as improvement indicator')
    then('test run always succeeds regardless of delta')
      sothat('reporter is purely informational')

given('baseline file configured but absent')
  when('tests complete')
    then('info message displayed: baseline not found')
    then('current report can serve as new baseline')
    then('test run succeeds')
      sothat('first run bootstraps baseline gracefully')
```

## usecase.5 = configure thresholds

```
given('slow threshold configured in milliseconds')
  when('tests complete')
    then('threshold applied as configured')

given('slow threshold configured as human-readable string')
  when('tests complete')
    then('strings like "1s", "500ms", "10s" parsed correctly')
      sothat('config is readable without mental math')

given('no threshold configured')
  when('tests complete')
    then('sensible default applied: slow=3000ms for unit, 10000ms for integration/acceptance')
      sothat('zero-config experience works out of box')
```

## usecase.6 = handle edge cases

```
given('empty test file')
  when('tests complete')
    then('file reported with 0ms duration')
    then('no error thrown')

given('test file that throws in setup')
  when('tests complete')
    then('file duration captured up to failure point')
    then('reporter does not crash')

given('1000+ test files')
  when('tests complete')
    then('terminal output paginated or truncated')
    then('json output contains all files')
      sothat('terminal stays readable while json stays complete')

given('tests run in parallel')
  when('tests complete')
    then('per-file duration reflects wall clock for that file')
    then('per-test duration reflects isolated execution time')
      sothat('parallel overhead doesnt inflate individual test times')

given('identical test names in different files')
  when('tests complete')
    then('each test distinguished by file path')
    then('no collision in json output')
```
